"""
Azure OpenAI LLM Service for Search Response Generation
"""

import logging
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime

import os
import sys

# Set comprehensive Pydantic compatibility mode before importing OpenAI
os.environ.setdefault('PYDANTIC_V1_COMPAT', '1')
os.environ.setdefault('PYDANTIC_V1', '1')

# Suppress Pydantic warnings globally
import warnings
warnings.filterwarnings("ignore", message="Field .* has conflict with protected namespace")
warnings.filterwarnings("ignore", message="Fields must not use names with leading underscores")

from openai import AzureOpenAI

from app.config import settings
from app.prompts import (
    MULTI_DEPT_HINT,
    SYSTEM_PROMPT,
    ATTACHMENTS_HINT,
    INSTRUCTION_CONTEXT_DEP,
    INSTRUCTION_DEFAULT,
)
from app.core.mem0_client import Mem0Service

logger = logging.getLogger(__name__)


class AzureLLMService:
    """Azure OpenAI LLM service for generating responses from search results"""
    
    def __init__(self):
        self.client: Optional[AzureOpenAI] = None
        self.deployment_name = "gpt-4.1-mini"
        self.model_name = "gpt-4.1-mini"
        self.initialized = False
        
        self.mem0_service: Optional[Mem0Service] = None
        if settings.MEM0_ENABLED:
            try:
                self.mem0_service = Mem0Service(base_url=settings.MEM0_BASE_URL)
            except Exception as e:
                logger.warning(f"Failed to initialize Mem0 service: {e}")
                self.mem0_service = None
    
    async def initialize(self) -> bool:
        """Initialize Azure OpenAI client"""
        
        if self.initialized:
            return True
        
        try:
            import warnings
            warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

            self.client = AzureOpenAI(
                api_key=settings.OPENAI_API_KEY,
                api_version=settings.AZURE_OPENAI_API_VERSION,
                azure_endpoint=settings.AZURE_OPENAI_ENDPOINT
            )

            test_response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )

            if test_response.choices and len(test_response.choices) > 0:
                self.initialized = True
                return True
            else:
                raise Exception("No completion data returned from test")

        except Exception as e:
            logger.error(f"Failed to initialize Azure LLM service: {e}")
            if "pydantic" in str(e).lower() or "__pydantic" in str(e):
                try:
                    os.environ['PYDANTIC_V1'] = '1'
                    import importlib
                    import openai
                    importlib.reload(openai)

                    self.client = AzureOpenAI(
                        api_key=settings.OPENAI_API_KEY,
                        api_version=settings.AZURE_OPENAI_API_VERSION,
                        azure_endpoint=settings.AZURE_OPENAI_ENDPOINT
                    )

                    test_response = self.client.chat.completions.create(
                        model=self.deployment_name,
                        messages=[{"role": "user", "content": "Hello"}],
                        max_tokens=10
                    )

                    if test_response.choices and len(test_response.choices) > 0:
                        self.initialized = True
                        return True
                except Exception as fallback_error:
                    logger.error(f"Fallback approach also failed: {fallback_error}")

            raise Exception(f"Azure LLM initialization failed: {e}")
    
    def _is_context_dependent_question(self, query: str) -> bool:
        """Check if the query is context-dependent (refers to previous conversation)"""
        query_lower = query.lower().strip()
        
        context_patterns = [
            # Pronouns
            "ê·¸ê±°", "ê·¸ê²ƒ", "ê·¸ê²Œ", "ê·¸ê±´", "ê·¸ê±´ê°€", "ì €ê±°", "ì €ê²ƒ", "ì´ê±°", "ì´ê²ƒ",
            # Temporal references
            "ë°©ê¸ˆ", "ì´ì „ì—", "ì „ì—", "ì•„ê¹Œ", "ì¡°ê¸ˆ ì „", "ì•žì—ì„œ",
            # Question patterns
            "ì–´ë–»ê²Œ", "ì–´ë–»ê²Œ í•´ì•¼", "ì–´ë–»ê²Œ í•´ì•¼ë˜ëŠ”ë°", "ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ë°",
            "ë­ë¬¼ì–´ë´¤", "ë­ë¬¼ì–´ë´¤ë”ë¼", "ë­ë¬¼ì–´ë´¤ì§€", "ë­ë¬¼ì–´ë´¤ì–´",
            "ë­í–ˆ", "ë­í–ˆë”ë¼", "ë­í–ˆì§€", "ë­í–ˆì–´",
            "ë­ë§í–ˆ", "ë­ë§í–ˆë”ë¼", "ë­ë§í–ˆì§€", "ë­ë§í–ˆì–´",
            "ë­ëŒ€í™”í–ˆ", "ë­ëŒ€í™”í–ˆë”ë¼", "ë­ëŒ€í™”í–ˆì§€", "ë­ëŒ€í™”í–ˆì–´",
            # References
            "ê·¸ ì§ˆë¬¸", "ê·¸ ë‹µë³€", "ê·¸ ë‚´ìš©", "ê·¸ ì •ë³´",
            # Numbered references
            "ë²ˆ", "ì²« ë²ˆì§¸", "ë‘ ë²ˆì§¸", "ì„¸ ë²ˆì§¸", "ë§ˆì§€ë§‰",
            # Repetition
            "ë‹¤ì‹œ", "ë‹¤ì‹œ ë§í•´", "ë‹¤ì‹œ ì„¤ëª…", "ë‹¤ì‹œ ì•Œë ¤",
            # Memory issues
            "ìžŠì—ˆ", "ê¸°ì–µ ì•ˆë‚˜", "ê¸°ì–µì´ ì•ˆë‚˜", "ëª¨ë¥´ê² ",
            "ë­ì˜€", "ë­ì˜€ë”ë¼", "ë­ì˜€ì§€", "ë­ì˜€ì–´"
        ]
        
        return any(pattern in query_lower for pattern in context_patterns)

    async def generate_response_with_context(
        self, 
        query: str, 
        search_results: List[Dict[str, Any]], 
        chat_context: Optional[Dict[str, Any]] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        searched_collections: Optional[List[str]] = None,
        user_id: Optional[int] = None,
        session_id: Optional[str] = None,
        active_collection: Optional[str] = None
    ) -> str:
        """Generate a response based on search results, user query, and chat context"""


        if not self.initialized:
            logger.info("Initializing Azure LLM service...")
            init_result = await self.initialize()
            logger.info(f"Initialization result: {init_result}")

        if not query.strip():
            logger.warning("Empty query provided")
            return "ì§ˆë¬¸ì„ í•´ì£¼ì…”ì•¼ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."

        try:
            memory_context = ""
            if self.mem0_service and user_id and session_id:
                try:
                    combined_memories = await self.mem0_service.search_combined_memories(
                        query=query,
                        user_id=user_id,
                        session_id=session_id,
                        user_limit=settings.MEM0_USER_MEMORY_LIMIT,
                        session_limit=settings.MEM0_SESSION_MEMORY_LIMIT
                    )
                    
                    if combined_memories.get("user_memories"):
                        user_memories_list = []
                        for memory in combined_memories["user_memories"]:
                            if isinstance(memory, dict):
                                memory_text = memory.get("memory", "")
                                if memory_text:
                                    user_memories_list.append(f"- {memory_text}")
                        
                        if user_memories_list:
                            memory_context += f"\n\nðŸ“ ì‚¬ìš©ìž ê°œì¸ ì •ë³´ (ìž¥ê¸° ê¸°ì–µ):\n" + "\n".join(user_memories_list)
                    
                    if combined_memories.get("session_memories"):
                        session_memories_list = []
                        for memory in combined_memories["session_memories"]:
                            if isinstance(memory, dict):
                                memory_text = memory.get("memory", "")
                                if memory_text:
                                    session_memories_list.append(f"- {memory_text}")
                        
                        if session_memories_list:
                            memory_context += f"\n\nðŸ’­ ì´ë²ˆ ëŒ€í™” ë§¥ë½ (ë‹¨ê¸° ê¸°ì–µ):\n" + "\n".join(session_memories_list)
                        
                except Exception as e:
                    logger.warning(f"Failed to search Mem0 memories: {e}")
                    memory_context = ""
            
            # Prepare context from search results
            context_parts = []
            has_search_results = search_results and len(search_results) > 0
            
            if has_search_results:
                for i, result in enumerate(search_results[:5], 1):  # Limit to top 5 results
                    content = result.get('content', '')
                    source = result.get('source_file', 'Unknown source')
                    score = result.get('score', 0.0)
                    
                    context_parts.append(f"Source {i} (from {source}, relevance: {score:.2f}):\n{content}\n")

            search_context = "\n".join(context_parts) if context_parts else ""

            chat_history = []
            context_summary = ""
            current_department = None
            
            if chat_context:
                chat_history_raw = chat_context.get("chat_history", [])
                if chat_history_raw and isinstance(chat_history_raw, list):
                    # Limit to exactly 3 turns (6 messages: 3 user + 3 assistant)
                    # Take the last 6 messages to ensure we have the most recent context
                    limited_history = chat_history_raw[-6:] if len(chat_history_raw) > 6 else chat_history_raw
                    for msg in limited_history:
                        if isinstance(msg, dict) and "role" in msg and "content" in msg:
                            chat_history.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })
                
                context_summary = chat_context.get("context_summary", "")
                current_department = chat_context.get("current_department")

            system_prompt = SYSTEM_PROMPT
            is_context_dependent = self._is_context_dependent_question(query)
            
            # Build user prompt with context
            user_prompt_parts = [f"ì‚¬ìš©ìž ë©”ì‹œì§€: {query}"]
            
            # ì²¨ë¶€ëœ í…ìŠ¤íŠ¸/ì½”ë“œ ìŠ¤ë‹ˆíŽ«ì´ ìžˆìœ¼ë©´ ì‚¬ìš©ìž ë©”ì‹œì§€ ë°”ë¡œ ë‹¤ìŒì— í¬í•¨ (ëª…í™•ì„±ì„ ìœ„í•´)
            attachments_text = []
            if chat_context:
                try:
                    attachments_text = chat_context.get("attachments_text", []) or []
                    logger.info(f"Found attachments_text in chat_context: {len(attachments_text)} items")
                except Exception as e:
                    logger.warning(f"Failed to extract attachments_text from chat_context: {e}")
                    attachments_text = []

            if attachments_text:
                try:
                    # Build a concise attached context block
                    attached_parts = []
                    file_names = []
                    for att in attachments_text[:5]:
                        name = att.get('name', 'attachment') if isinstance(att, dict) else 'attachment'
                        snippet = att.get('snippet', '') if isinstance(att, dict) else ''
                        if snippet:
                            file_names.append(name)
                            attached_parts.append(f"[íŒŒì¼: {name}]\n{snippet[:2000]}")
                    if attached_parts:
                        files_str = ", ".join(file_names)
                        user_prompt_parts.append(f"\nì‚¬ìš©ìžê°€ ë‹¤ìŒ íŒŒì¼ì„ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤: {files_str}")
                        user_prompt_parts.append("\nì²¨ë¶€ëœ íŒŒì¼ ë‚´ìš©:\n" + "\n\n".join(attached_parts))
                        user_prompt_parts.append("\n" + ATTACHMENTS_HINT)
                except Exception:
                    pass
            
            if context_summary:
                user_prompt_parts.append(f"\nì´ì „ ëŒ€í™” ìš”ì•½:\n{context_summary}")
            
            # Add memory context from Mem0 (user preferences and session memories)
            if memory_context:
                user_prompt_parts.append(memory_context)
            
            # For context-dependent questions, rely on mem0 session memories for context
            # Mem0 session memories should contain recent conversation context
            if is_context_dependent and current_department:
                user_prompt_parts.append(f"\nì¤‘ìš” ì°¸ê³ : í˜„ìž¬ ì§ˆë¬¸ì€ {current_department} ì»¬ë ‰ì…˜ì— ëŒ€í•œ ê²ƒìž…ë‹ˆë‹¤. 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê±°' ë“±ì˜ í‘œí˜„ì€ ì´ ì»¬ë ‰ì…˜ì˜ ë‚´ìš©ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.")

            if search_context:
                user_prompt_parts.append(f"\nì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ (ê´€ë ¨ì„±ì´ ìžˆì„ ìˆ˜ë„ ì—†ì„ ìˆ˜ë„ ìžˆìŒ):\n{search_context}")
            elif active_collection:
                user_prompt_parts.append(
                    "\nì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼: í•´ë‹¹ ì»¬ë ‰ì…˜ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                    "ì‘ë‹µì„ 'ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤'ë¼ëŠ” ë¬¸ìž¥ìœ¼ë¡œ ì‹œìž‘í•˜ê³ , í•„ìš”í•œ ê²½ìš° í›„ì† ì§ˆë¬¸ì´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ë§Œ ì œì•ˆí•˜ì„¸ìš”."
                )
            
            # Add multi-department hint if multiple collections were searched
            if searched_collections and len(searched_collections) > 1:
                collections_str = ", ".join(searched_collections)
                user_prompt_parts.append(f"\n{MULTI_DEPT_HINT.format(collections=collections_str)}")
            
            # Enhanced instruction for context-dependent questions
            instruction_block = INSTRUCTION_CONTEXT_DEP if is_context_dependent else INSTRUCTION_DEFAULT
            user_prompt_parts.append("\n" + instruction_block)

            user_prompt = "\n".join(user_prompt_parts)

            # Build messages array with sliding window chat history
            messages = [{"role": "system", "content": system_prompt}]
            
            if chat_history:
                messages.extend(chat_history)
            
            messages.append({"role": "user", "content": user_prompt})
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )

            if response.choices and len(response.choices) > 0:
                generated_text = response.choices[0].message.content
                
                if self.mem0_service and user_id and session_id and generated_text:
                    try:
                        response_len_check = len(generated_text) > 200
                        keyword_check = any(keyword in query.lower() for keyword in ['ì¢‹ì•„', 'ì‹«ì–´', 'ì„ í˜¸', 'ì·¨í–¥', 'ìŠµê´€'])
                        auto_save_check = settings.MEM0_AUTO_SAVE_IMPORTANT
                        
                        is_important = (
                            response_len_check or
                            keyword_check or
                            auto_save_check
                        )
                        
                        await self.mem0_service.add_conversation_memory(
                            query=query,
                            response=generated_text,
                            user_id=user_id,
                            session_id=session_id,
                            is_important=is_important
                        )
                    except Exception as e:
                        logger.warning(f"Failed to save conversation to memory: {e}")
                
                return generated_text
            else:
                logger.error("No completion data returned from Azure")
                raise Exception("No completion data returned")

        except Exception as e:
            logger.error(f"Response generation with context failed: {e}")
            # Safe fallback on Azure content filter or Responsible AI violations
            err = str(e).lower()
            if "content_filter" in err or "responsibleaipolicyviolation" in err:
                try:
                    logger.warning("Content filter triggered. Retrying with safe minimal prompt...")
                    safe_system = "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ì•ˆì „í•˜ê³  ì¡´ì¤‘í•˜ëŠ” í‘œí˜„ìœ¼ë¡œ ê°„ë‹¨ížˆ ì•ˆë‚´í•©ë‹ˆë‹¤."
                    safe_user = (
                        f"ì‚¬ìš©ìž ë©”ì‹œì§€: {query}\n\n"
                        "ìž‘ì—…: í˜„ìž¬ ë©”ì‹œì§€ì— ëŒ€í•´ ê°„ë‹¨í•˜ê³  ì•ˆì „í•œ ë²”ìœ„ì—ì„œ ë‹µë³€í•˜ì„¸ìš”."
                    )
                    safe_resp = self.client.chat.completions.create(
                        model=self.deployment_name,
                        messages=[
                            {"role": "system", "content": safe_system},
                            {"role": "user", "content": safe_user},
                        ],
                        max_tokens=min(400, max_tokens),
                        temperature=min(0.4, max(0.2, temperature)),
                    )
                    if safe_resp.choices and len(safe_resp.choices) > 0:
                        return safe_resp.choices[0].message.content
                except Exception as safe_ex:
                    logger.error(f"Safe retry failed: {safe_ex}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ìš”ì²­ì´ ì •ì±…ì— ì˜í•´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            raise Exception(f"Failed to generate response: {e}")

    async def generate_streaming_response_with_context(
        self, 
        query: str, 
        context_results: List[Any], 
        chat_context: Optional[Dict[str, Any]] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        searched_collections: Optional[List[str]] = None,
        user_id: Optional[int] = None,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """Generate a streaming response based on search results, user query, and chat context"""

        if not self.initialized:
            await self.initialize()

        if not query.strip():
            logger.warning("Empty query provided")
            yield "ì§ˆë¬¸ì„ í•´ì£¼ì…”ì•¼ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
            return
        
        if not context_results:
            logger.warning("No search results provided")
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return
        
        try:
            memory_context = ""
            if self.mem0_service and user_id and session_id:
                try:
                    combined_memories = await self.mem0_service.search_combined_memories(
                        query=query,
                        user_id=user_id,
                        session_id=session_id,
                        user_limit=settings.MEM0_USER_MEMORY_LIMIT,
                        session_limit=settings.MEM0_SESSION_MEMORY_LIMIT
                    )
                    
                    if combined_memories.get("user_memories"):
                        user_memories_list = []
                        for memory in combined_memories["user_memories"]:
                            if isinstance(memory, dict):
                                memory_text = memory.get("memory", "")
                                if memory_text:
                                    user_memories_list.append(f"- {memory_text}")
                        
                        if user_memories_list:
                            memory_context += f"\n\nðŸ“ ì‚¬ìš©ìž ê°œì¸ ì •ë³´ (ìž¥ê¸° ê¸°ì–µ):\n" + "\n".join(user_memories_list)
                    
                    if combined_memories.get("session_memories"):
                        session_memories_list = []
                        for memory in combined_memories["session_memories"]:
                            if isinstance(memory, dict):
                                memory_text = memory.get("memory", "")
                                if memory_text:
                                    session_memories_list.append(f"- {memory_text}")
                        
                        if session_memories_list:
                            memory_context += f"\n\nðŸ’­ ì´ë²ˆ ëŒ€í™” ë§¥ë½ (ë‹¨ê¸° ê¸°ì–µ):\n" + "\n".join(session_memories_list)
                        
                except Exception as e:
                    logger.warning(f"Failed to search Mem0 memories: {e}")
                    memory_context = ""
            
            # Prepare context from search results
            context_parts = []
            for i, result in enumerate(context_results):
                if hasattr(result, 'content'):
                    content = result.content
                    source = getattr(result, 'source_file', f'Result {i+1}')
                    context_parts.append(f"[Source: {source}]\n{content}")
                elif isinstance(result, dict):
                    content = result.get('content', '')
                    source = result.get('source_file', f'Result {i+1}')
                    context_parts.append(f"[Source: {source}]\n{content}")
                else:
                    context_parts.append(f"[Result {i+1}]\n{str(result)}")

            search_context = "\n".join(context_parts)

            # Extract chat history for sliding window context
            chat_history = []
            context_summary = ""
            current_department = None
            
            if chat_context:
                chat_history_raw = chat_context.get("chat_history", [])
                if chat_history_raw and isinstance(chat_history_raw, list):
                    # Limit to exactly 3 turns (6 messages: 3 user + 3 assistant)
                    # Take the last 6 messages to ensure we have the most recent context
                    limited_history = chat_history_raw[-6:] if len(chat_history_raw) > 6 else chat_history_raw
                    for msg in limited_history:
                        if isinstance(msg, dict) and "role" in msg and "content" in msg:
                            chat_history.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })
                
                context_summary = chat_context.get("context_summary", "")
                current_department = chat_context.get("current_department")

            # Create the enhanced prompt
            system_prompt = """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œ ìžì—°ìŠ¤ëŸ½ê²Œ í•´ì£¼ì„¸ìš”.

ì‘ë‹µ ë°©ì‹:
1. í˜„ìž¬ ì‚¬ìš©ìž ë©”ì‹œì§€ë¥¼ ê°€ìž¥ ìš°ì„ ìœ¼ë¡œ ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”
2. ì´ì „ ëŒ€í™” ë§¥ë½ì€ ë³´ì¡° ì»¨í…ìŠ¤íŠ¸ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš” (í•„ìš” ì‹œ)
3. ê²€ìƒ‰ ê²°ê³¼ê°€ ì‚¬ìš©ìž ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìžˆëŠ”ì§€ ë¨¼ì € íŒë‹¨í•˜ì„¸ìš”
4. ê´€ë ¨ì„±ì´ ìžˆë‹¤ë©´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”
5. ê´€ë ¨ì„±ì´ ì—†ë‹¤ë©´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ê³  ìžì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”
6. ì¸ì‚¬ë‚˜ ì¼ë°˜ ëŒ€í™”ì—ëŠ” ë”°ëœ»í•˜ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”
7. ê¸°ìˆ ì  ì§ˆë¬¸ì—ëŠ” ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
8. ì›ì‹œ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ì§€ ë§ê³  ì‚¬ìš©ìžê°€ ì´í•´í•  ìˆ˜ ìžˆë„ë¡ ì„¤ëª…í•˜ì„¸ìš”

ì¤‘ìš”: 
- ì‚¬ìš©ìžì˜ ì‹¤ì œ ì§ˆë¬¸ì— ë¨¼ì € ì§‘ì¤‘í•˜ì„¸ìš”. ê²€ìƒ‰ ê²°ê³¼ì™€ ì´ì „ ëŒ€í™”ëŠ” ì°¸ê³ ìš©ìž…ë‹ˆë‹¤.
- ì¸ì‚¬ë‚˜ ì¼ë°˜ ëŒ€í™”ì—ëŠ” ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€ ë¨¼ì € íŒŒì•…í•˜ì„¸ìš”."""

            # Check if query is context-dependent
            is_context_dependent = self._is_context_dependent_question(query)
            
            # Build user prompt with context
            user_prompt_parts = [f"ì‚¬ìš©ìž ë©”ì‹œì§€: {query}"]
            
            # ì²¨ë¶€ëœ í…ìŠ¤íŠ¸/ì½”ë“œ ìŠ¤ë‹ˆíŽ«ì´ ìžˆìœ¼ë©´ ì‚¬ìš©ìž ë©”ì‹œì§€ ë°”ë¡œ ë‹¤ìŒì— í¬í•¨ (ëª…í™•ì„±ì„ ìœ„í•´)
            attachments_text = []
            if chat_context:
                try:
                    attachments_text = chat_context.get("attachments_text", []) or []
                    logger.info(f"Found attachments_text in chat_context: {len(attachments_text)} items")
                except Exception as e:
                    logger.warning(f"Failed to extract attachments_text from chat_context: {e}")
                    attachments_text = []

            if attachments_text:
                try:
                    attached_parts = []
                    file_names = []
                    for att in attachments_text[:5]:
                        name = att.get('name', 'attachment') if isinstance(att, dict) else 'attachment'
                        snippet = att.get('snippet', '') if isinstance(att, dict) else ''
                        if snippet:
                            file_names.append(name)
                            attached_parts.append(f"[íŒŒì¼: {name}]\n{snippet[:2000]}")
                    if attached_parts:
                        files_str = ", ".join(file_names)
                        user_prompt_parts.append(f"\nì‚¬ìš©ìžê°€ ë‹¤ìŒ íŒŒì¼ì„ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤: {files_str}")
                        user_prompt_parts.append("\nì²¨ë¶€ëœ íŒŒì¼ ë‚´ìš©:\n" + "\n\n".join(attached_parts))
                        user_prompt_parts.append("\nì§€ì¹¨: ì²¨ë¶€ ìžë£ŒëŠ” ì°¸ê³ ìš©ìž…ë‹ˆë‹¤. í˜„ìž¬ ì‚¬ìš©ìž ì§ˆë¬¸ì— ìš°ì„  ì§ì ‘ ë‹µë³€í•˜ì„¸ìš”. ì²¨ë¶€ ë‚´ìš©ì´ ë„ì›€ì´ ë  ë•Œë§Œ ìš”ì  ìœ„ì£¼ë¡œ ë°˜ì˜í•˜ê³ , ê´€ë ¨ì„±ì´ ì•½í•˜ë©´ ê³¼ê°ížˆ ë¬´ì‹œí•˜ì„¸ìš”.")
                except Exception:
                    pass
            
            if context_summary:
                user_prompt_parts.append(f"\nì´ì „ ëŒ€í™” ìš”ì•½:\n{context_summary}")
            
            # Add memory context from Mem0 (user preferences and session memories)
            if memory_context:
                user_prompt_parts.append(memory_context)
            
            # For context-dependent questions, rely on mem0 session memories for context
            # Mem0 session memories should contain recent conversation context
            if is_context_dependent and current_department:
                user_prompt_parts.append(f"\nì¤‘ìš” ì°¸ê³ : í˜„ìž¬ ì§ˆë¬¸ì€ {current_department} ì»¬ë ‰ì…˜ì— ëŒ€í•œ ê²ƒìž…ë‹ˆë‹¤. 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê±°' ë“±ì˜ í‘œí˜„ì€ ì´ ì»¬ë ‰ì…˜ì˜ ë‚´ìš©ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.")

            if search_context:
                user_prompt_parts.append(f"\nì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ (ê´€ë ¨ì„±ì´ ìžˆì„ ìˆ˜ë„ ì—†ì„ ìˆ˜ë„ ìžˆìŒ):\n{search_context}")
            
            # Add multi-department hint if multiple collections were searched
            if searched_collections and len(searched_collections) > 1:
                collections_str = ", ".join(searched_collections)
                user_prompt_parts.append(f"\n{MULTI_DEPT_HINT.format(collections=collections_str)}")
            
            # Enhanced instruction for context-dependent questions
            instruction_block = INSTRUCTION_CONTEXT_DEP if is_context_dependent else INSTRUCTION_DEFAULT
            user_prompt_parts.append("\n" + instruction_block)

            user_prompt = "\n".join(user_prompt_parts)

            # Build messages array with sliding window chat history
            messages = [{"role": "system", "content": system_prompt}]
            
            if chat_history:
                messages.extend(chat_history)
            
            messages.append({"role": "user", "content": user_prompt})
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True
            )

            # Stream the response chunks
            async for chunk in response:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        yield delta.content
            
            # Note: Memory saving is handled in the route handler after streaming completes
            # to avoid blocking the streaming response or sending data after connection closes
            # The route handler accumulates the full response and saves it in a background task

        except Exception as e:
            logger.error(f"Streaming response generation with context failed: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def generate_response(
        self, 
        query: str, 
        search_results: List[Dict[str, Any]], 
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> str:
        """Generate a response based on search results and user query"""

        logger.info(f"generate_response called with query: '{query}', results_count: {len(search_results)}, initialized: {self.initialized}")

        if not self.initialized:
            logger.info("Initializing Azure LLM service...")
            init_result = await self.initialize()
            logger.info(f"Initialization result: {init_result}")

        if not query.strip():
            logger.warning("Empty query provided")
            return "ì§ˆë¬¸ì„ í•´ì£¼ì…”ì•¼ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."

        try:
            # Prepare context from search results
            context_parts = []
            has_search_results = search_results and len(search_results) > 0
            
            if has_search_results:
                for i, result in enumerate(search_results[:5], 1):  # Limit to top 5 results
                    content = result.get('content', '')
                    source = result.get('source_file', 'Unknown source')
                    score = result.get('score', 0.0)
                    
                    context_parts.append(f"Source {i} (from {source}, relevance: {score:.2f}):\n{content}\n")

            context = "\n".join(context_parts) if context_parts else ""

            # Create the prompt with different messages based on search results
            if not has_search_results:
                # No search results - pure conversational mode
                system_prompt = """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œ ìžì—°ìŠ¤ëŸ½ê²Œ í•´ì£¼ì„¸ìš”.

ì‘ë‹µ ë°©ì‹:
1. ì‚¬ìš©ìžì˜ ì˜ë„ë¥¼ ì •í™•ížˆ íŒŒì•…í•˜ì„¸ìš”
2. ì¸ì‚¬ë‚˜ ì¼ë°˜ ëŒ€í™”ì—ëŠ” ë”°ëœ»í•˜ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”
3. ê¸°ìˆ ì  ì§ˆë¬¸ì—ëŠ” ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
4. íŠ¹ì • ê¸°ìˆ  ë¬¸ì„œë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”

ì¤‘ìš”: 
- ìžì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ëŒ€í™”ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
- ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€ ë¨¼ì € íŒŒì•…í•˜ê³  ë„ì›€ì„ ì œê³µí•˜ì„¸ìš”."""

                user_prompt = f"""ì‚¬ìš©ìž ë©”ì‹œì§€: {query}

ìž‘ì—…: ì‚¬ìš©ìžì˜ ë©”ì‹œì§€ì— ìžì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”. ëŒ€í™”í•˜ë“¯ì´ ë”°ëœ»í•˜ê³  ë„ì›€ì´ ë˜ë„ë¡ ì‘ë‹µí•˜ì„¸ìš”."""
            else:
                # Has search results - RAG mode
                system_prompt = """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.

ì‘ë‹µ ë°©ì‹:
1. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ ì •í™•ížˆ ì´í•´í•˜ì„¸ìš”
2. ê²€ìƒ‰ ê²°ê³¼ê°€ ê´€ë ¨ì„±ì´ ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
3. ê´€ë ¨ì„±ì´ ìžˆë‹¤ë©´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”
4. ê´€ë ¨ì„±ì´ ì—†ë‹¤ë©´ (ì¸ì‚¬, ì¼ë°˜ ëŒ€í™” ë“±) ìžì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•˜ì„¸ìš”
5. ìžì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²˜ëŸ¼ ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
6. ì›ì‹œ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ì§€ ë§ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ìš©ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”
7. ì‹¤ì œë¡œ ì‚¬ìš©í•œ ì •ë³´ì— ëŒ€í•´ì„œë§Œ ì¶œì²˜ë¥¼ ì¸ìš©í•˜ì„¸ìš”

ì¤‘ìš”: ê²€ìƒ‰ ê²°ê³¼ê°€ í•­ìƒ ì‚¬ìš©ìžì˜ ë©”ì‹œì§€ì™€ ê´€ë ¨ì´ ìžˆëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. íŒë‹¨ë ¥ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ì–µì§€ë¡œ ì‘ë‹µì— í¬í•¨ì‹œí‚¤ì§€ ë§ˆì„¸ìš”."""

                user_prompt = f"""ì‚¬ìš©ìž ë©”ì‹œì§€: {query}

ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ (ì°¸ê³ ìš©):

{context}

ìž‘ì—…: ì‚¬ìš©ìžì˜ ë©”ì‹œì§€ì— ìžì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”. ê²€ìƒ‰ ê²°ê³¼ê°€ ì‚¬ìš©ìž ì§ˆë¬¸ê³¼ ê´€ë ¨ ìžˆê³  ë„ì›€ì´ ëœë‹¤ë©´ ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ê´€ë ¨ì´ ì—†ë‹¤ë©´ (ì˜ˆ: ì¸ì‚¬, ì¼ë°˜ ëŒ€í™”) ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ê³  ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ëŒ€ë¡œ ì‘ë‹µí•˜ì„¸ìš”. ëŒ€í™”í•˜ë“¯ì´ ë”°ëœ»í•˜ê³  ë„ì›€ì´ ë˜ë„ë¡ ì‘ë‹µí•˜ì„¸ìš”."""

            logger.info(f"Creating completion with model: {self.deployment_name}")
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )

            if response.choices and len(response.choices) > 0:
                generated_text = response.choices[0].message.content
                return generated_text
            else:
                logger.error("No completion data returned from Azure")
                raise Exception("No completion data returned")

        except Exception as e:
            logger.error(f"Response generation failed: {e}")
            raise Exception(f"Failed to generate response: {e}")
    
    async def generate_streaming_response(
        self, 
        query: str, 
        context_results: List[Any], 
        max_tokens: int = 1000,
        temperature: float = 0.7
    ):
        """Generate a streaming response based on search results and user query"""
        
        if not self.initialized:
            await self.initialize()
        
        if not query.strip():
            logger.warning("Empty query provided")
            yield "ì§ˆë¬¸ì„ í•´ì£¼ì…”ì•¼ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
            return
        
        if not context_results:
            logger.warning("No search results provided")
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return
        
        try:
            # Prepare context from search results
            context_parts = []
            for i, result in enumerate(context_results):
                if hasattr(result, 'content'):
                    content = result.content
                    source = getattr(result, 'source_file', f'Result {i+1}')
                    context_parts.append(f"[Source: {source}]\n{content}")
                elif isinstance(result, dict):
                    content = result.get('content', '')
                    source = result.get('source_file', f'Result {i+1}')
                    context_parts.append(f"[Source: {source}]\n{content}")
            
            context = "\n\n".join(context_parts)
            
            # Create system prompt
            system_prompt = """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œ ìžì—°ìŠ¤ëŸ½ê²Œ í•´ì£¼ì„¸ìš”.
            ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ë§žëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
            ì£¼ì–´ì§„ ë§¥ë½ì´ ê´€ë ¨ì´ ìžˆë‹¤ë©´ ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
            ë§¥ë½ì´ ê´€ë ¨ì´ ì—†ë‹¤ë©´ ìžì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.
            ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""
            
            # Create user prompt
            user_prompt = f"""ì°¸ê³  ë§¥ë½:
{context}

ì§ˆë¬¸: {query}

ìœ„ì˜ ë§¥ë½ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìžˆë‹¤ë©´ ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”. ê´€ë ¨ì´ ì—†ë‹¤ë©´ ìžì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ ì£¼ì„¸ìš”."""
            
            logger.info(f"Creating streaming completion with model: {self.deployment_name}")
            
            # Create streaming completion
            stream = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True
            )
            
            # Stream the response
            for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content
            
            logger.info("Streaming response completed successfully")
            
        except Exception as e:
            logger.error(f"Streaming response generation failed: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def generate_title(self, message: str, max_tokens: int = 50, temperature: float = 0.3) -> str:
        """Generate a chat title from user message"""
        
        if not self.initialized or not self.client:
            raise Exception("Azure LLM service not initialized")
        
        try:
            logger.info(f"Generating title for message: {message[:50]}...")
            
            # ì œëª© ìƒì„± í”„ë¡¬í”„íŠ¸
            prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ìž ë©”ì‹œì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì±„íŒ…ë°© ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ìž ë©”ì‹œì§€: "{message}"

ìš”êµ¬ì‚¬í•­:
- 12ìž ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìž‘ì„±
- ë©”ì‹œì§€ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë°˜ì˜
- í•œêµ­ì–´ë¡œ ìž‘ì„±
- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë‚˜ ì¸ì‚¬ë§ ì œì™¸
- ì§ˆë¬¸ì˜ í•µì‹¬ë§Œ ì¶”ì¶œ

ì œëª©ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
 
ì¶”ê°€ ê·œì¹™:
- ìž…ë ¥ ì „ì²˜ë¦¬: ì•žë’¤ ê³µë°±ê³¼ ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ìž(ì˜ˆ: ~, !, ?, ì´ëª¨ì§€)ë¥¼ ì œê±°í•œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
- ê¸¸ì´ ê·œì¹™:
  - ì²˜ë¦¬ ê¸°ì¤€ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ 5ìž ì´í•˜ì´ë©´: ì œëª© = ì²˜ë¦¬ ê¸°ì¤€ í…ìŠ¤íŠ¸ + " ì±„íŒ…ë°©"
    ì˜ˆ: "ì•ˆë…•" -> "ì•ˆë…• ì±„íŒ…ë°©", "ë°˜ê°€ì›Œìš”~" -> "ë°˜ê°€ì›Œìš” ì±„íŒ…ë°©"
  - ì²˜ë¦¬ ê¸°ì¤€ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ 6ìž ì´ìƒì´ë©´: ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì“°ì§€ ë§ê³ , í•µì‹¬ ì£¼ì œë¡œ ê°„ë‹¨ížˆ ë³€í™˜í•œ ì œëª©ìœ¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”. ì›ë¬¸ê³¼ ë™ì¼í•˜ê±°ë‚˜ ê±°ì˜ ë™ì¼í•œ í‘œí˜„ ê¸ˆì§€.
- ê³µí†µ ê·œì¹™:
  - 12ìž ì´ë‚´
  - í•œêµ­ì–´
  - ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´Â·ì¸ì‚¬ë§ ì œì™¸
  - ì œëª©ë§Œ ì¶œë ¥(ë”°ì˜´í‘œÂ·ë¶€ê°€ì„¤ëª… ê¸ˆì§€)

ì˜ˆì‹œ:
- ìž…ë ¥: "ì•ˆë…•" -> ì¶œë ¥: ì•ˆë…• ì±„íŒ…ë°©
- ìž…ë ¥: "ë°˜ê°€ì›Œìš”~" -> ì¶œë ¥: ë°˜ê°€ì›Œìš” ì±„íŒ…ë°©
- ìž…ë ¥: "ì˜¤ëŠ˜ íšŒì˜ ì•„ì  ë‹¤ ì •ë¦¬ ë¶€íƒí•´" -> ì¶œë ¥: íšŒì˜ ì•„ì  ë‹¤
- ìž…ë ¥: "ë§ˆìš°ìŠ¤ëŠ” ë¬´ìŠ¨ ì—­í• ì„ í•˜ë‚˜ìš”?" -> ì¶œë ¥: ë§ˆìš°ìŠ¤ì˜ ì—­í• 
"""
            
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if response.choices and len(response.choices) > 0:
                title = response.choices[0].message.content.strip()
                
                # ì œëª© ê¸¸ì´ ì œí•œ (12ìž) - ë‹¨ì–´ ê²½ê³„ì—ì„œ ìžë¥´ê¸°
                if len(title) > 12:
                    # 12ìž ì´ë‚´ì—ì„œ ë§ˆì§€ë§‰ ê³µë°± ì°¾ê¸°
                    lastSpaceIndex = title.rfind(' ', 0, 12)
                    if lastSpaceIndex > 0:
                        title = title[:lastSpaceIndex]
                    else:
                        # ê³µë°±ì´ ì—†ìœ¼ë©´ 12ìžë¡œ ìžë¥´ê¸°
                        title = title[:12]
                
                logger.info(f"Title generated successfully: {title}")
                return title
            else:
                logger.warning("No response from LLM for title generation")
                return "ìƒˆ ëŒ€í™”"
                
        except Exception as e:
            logger.error(f"Title generation failed: {e}")
            return "ìƒˆ ëŒ€í™”"
    
    async def close(self):
        """Clean up resources"""
        if self.client:
            # Azure OpenAI client doesn't need explicit cleanup
            self.client = None
            self.initialized = False
            logger.info("Azure LLM service closed")
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            "service": "azure_openai",
            "deployment": self.deployment_name,
            "model": self.model_name,
            "initialized": self.initialized
        }


