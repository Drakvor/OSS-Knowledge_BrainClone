"""
ì±„íŒ… ì œëª© ìƒì„± API ë¼ìš°í„°
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional
import structlog

from app.core.azure_llm import AzureLLMService

logger = structlog.get_logger(__name__)
router = APIRouter()

class TitleGenerationRequest(BaseModel):
    message: str
    language: str = "ko"

class TitleGenerationResponse(BaseModel):
    title: str
    success: bool
    error_message: Optional[str] = None

@router.post("/search/generate-title", response_model=TitleGenerationResponse)
async def generate_chat_title(request: TitleGenerationRequest):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì±„íŒ… ì œëª©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        logger.info("ğŸ¯ [TitleGeneration] Starting title generation", 
                   message=request.message[:50] + "...",
                   language=request.language)
        
        # Azure LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        llm_service = AzureLLMService()
        await llm_service.initialize()
        logger.info("ğŸ¤– [TitleGeneration] LLM service initialized")
        
        # ì œëª© ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì±„íŒ…ë°© ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€: "{request.message}"

ìš”êµ¬ì‚¬í•­:
- 20ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
- ë©”ì‹œì§€ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë°˜ì˜
- í•œêµ­ì–´ë¡œ ì‘ì„±
- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë‚˜ ì¸ì‚¬ë§ ì œì™¸
- ì§ˆë¬¸ì˜ í•µì‹¬ë§Œ ì¶”ì¶œ

ì œëª©ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
 
ì¶”ê°€ ê·œì¹™:
- ì…ë ¥ ì „ì²˜ë¦¬: ì•ë’¤ ê³µë°±ê³¼ ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì(ì˜ˆ: ~, !, ?, ì´ëª¨ì§€)ë¥¼ ì œê±°í•œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
- ê¸¸ì´ ê·œì¹™:
  - ì²˜ë¦¬ ê¸°ì¤€ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ 5ì ì´í•˜ì´ë©´: ì œëª© = ì²˜ë¦¬ ê¸°ì¤€ í…ìŠ¤íŠ¸ + " ì±„íŒ…ë°©"
    ì˜ˆ: "ì•ˆë…•" -> "ì•ˆë…• ì±„íŒ…ë°©", "ë°˜ê°€ì›Œìš”~" -> "ë°˜ê°€ì›Œìš” ì±„íŒ…ë°©"
  - ì²˜ë¦¬ ê¸°ì¤€ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ 6ì ì´ìƒì´ë©´: ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì“°ì§€ ë§ê³ , í•µì‹¬ ì£¼ì œë¡œ ê°„ë‹¨íˆ ë³€í™˜í•œ ì œëª©ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì›ë¬¸ê³¼ ë™ì¼í•˜ê±°ë‚˜ ê±°ì˜ ë™ì¼í•œ í‘œí˜„ ê¸ˆì§€.
- ê³µí†µ ê·œì¹™:
  - 12ì ì´ë‚´
  - í•œêµ­ì–´
  - ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´Â·ì¸ì‚¬ë§ ì œì™¸
  - ì œëª©ë§Œ ì¶œë ¥(ë”°ì˜´í‘œÂ·ë¶€ê°€ì„¤ëª… ê¸ˆì§€)

ì˜ˆì‹œ:
- ì…ë ¥: "ì•ˆë…•" -> ì¶œë ¥: ì•ˆë…• ì±„íŒ…ë°©
- ì…ë ¥: "ë°˜ê°€ì›Œìš”~" -> ì¶œë ¥: ë°˜ê°€ì›Œìš” ì±„íŒ…ë°©
- ì…ë ¥: "ì˜¤ëŠ˜ íšŒì˜ ì•„ì  ë‹¤ ì •ë¦¬ ë¶€íƒí•´" -> ì¶œë ¥: íšŒì˜ ì•„ì  ë‹¤
- ì…ë ¥: "ë§ˆìš°ìŠ¤ëŠ” ë¬´ìŠ¨ ì—­í• ì„ í•˜ë‚˜ìš”?" -> ì¶œë ¥: ë§ˆìš°ìŠ¤ì˜ ì—­í• 
"""
        
        logger.info("ğŸ“ [TitleGeneration] Calling LLM with prompt", prompt_length=len(prompt))
        
        # LLM í˜¸ì¶œ (ì œëª© ìƒì„± ì „ìš© ë©”ì„œë“œ ì‚¬ìš©)
        response = await llm_service.generate_title(
            message=request.message,
            max_tokens=50,
            temperature=0.3
        )
        
        logger.info("ğŸ¤– [TitleGeneration] LLM response received", 
                   response_length=len(response) if response else 0,
                   response_preview=response[:100] if response else "None")
        
        if response and response.strip():
            title = response.strip()
            
            logger.info("âœ… [TitleGeneration] Chat title generated successfully", 
                       original_message=request.message[:50], 
                       generated_title=title)
            
            return TitleGenerationResponse(
                title=title,
                success=True
            )
        else:
            logger.warning("âš ï¸ [TitleGeneration] Empty response from LLM for title generation")
            return TitleGenerationResponse(
                title="ìƒˆ ëŒ€í™”",
                success=False,
                error_message="ì œëª© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            )
            
    except Exception as e:
        logger.error("âŒ [TitleGeneration] Error generating chat title", error=str(e))
        return TitleGenerationResponse(
            title="ìƒˆ ëŒ€í™”",
            success=False,
            error_message=f"ì œëª© ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
